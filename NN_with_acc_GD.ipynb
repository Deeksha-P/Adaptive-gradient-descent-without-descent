{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NN with acc GD.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPSS8WKampktxeRn4NY/TuG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Deeksha-P/Adaptive-gradient-descent-without-descent/blob/master/NN_with_acc_GD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1n5hX_5ORvZq",
        "outputId": "e743ea54-2345-4702-d8ce-256f5733ffe7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Adaptive-gradient-descent-without-descent'...\n",
            "remote: Enumerating objects: 132, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 132 (delta 2), reused 0 (delta 0), pack-reused 126\u001b[K\n",
            "Receiving objects: 100% (132/132), 8.86 MiB | 11.88 MiB/s, done.\n",
            "Resolving deltas: 100% (59/59), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Deeksha-P/Adaptive-gradient-descent-without-descent.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "iYzFM1dOtLXY",
        "outputId": "38de9458-3a82-482d-e3ec-2a2172f51c89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xw7p3aMptPLm",
        "outputId": "0ac61cdb-17ce-448c-8789-ab5a78f455d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adaptive-gradient-descent-without-descent  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd Adaptive-gradient-descent-without-descent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhD-1zTwtQk3",
        "outputId": "54f47d3d-3996-43cd-a7fe-1b1c1f86d5fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Adaptive-gradient-descent-without-descent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "voyrqm3utTSq",
        "outputId": "21865a9c-d34b-4f10-b17d-23c30567c0eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/Adaptive-gradient-descent-without-descent'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from torch.optim.optimizer import Optimizer, required\n",
        "\n",
        "    \n",
        "class Adsgd(Optimizer):\n",
        "    def __init__(self, params, lr=0.2, amplifier=0.02, theta=1, damping=1, eps=1e-5, weight_decay=0):\n",
        "        if lr is not required and lr < 0.0:\n",
        "            raise ValueError(\"Invalid initial learning rate: {}\".format(lr))\n",
        "        if weight_decay < 0.0:\n",
        "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
        "\n",
        "        defaults = dict(lr=lr, amplifier=amplifier, theta=theta, damping=damping,\n",
        "                        eps=eps, weight_decay=weight_decay)\n",
        "        super(Adsgd, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(Adsgd, self).__setstate__(state)\n",
        "        for group in self.param_groups:\n",
        "            group.setdefault('lr', 0.2)\n",
        "            group.setdefault('amplifier', 0.02)\n",
        "            group.setdefault('damping', 1)\n",
        "            group.setdefault('theta', 1)\n",
        "                \n",
        "    def compute_dif_norms(self, prev_optimizer=required):\n",
        "        for group, prev_group in zip(self.param_groups, prev_optimizer.param_groups):\n",
        "            grad_dif_norm = 0\n",
        "            param_dif_norm = 0\n",
        "            for p, prev_p in zip(group['params'], prev_group['params']):\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                d_p = p.grad.data\n",
        "                prev_d_p = prev_p.grad.data\n",
        "                grad_dif_norm += (d_p - prev_d_p).norm().item() ** 2\n",
        "                param_dif_norm += (p.data - prev_p.data).norm().item() ** 2\n",
        "            group['grad_dif_norm'] = np.sqrt(grad_dif_norm)\n",
        "            group['param_dif_norm'] = np.sqrt(param_dif_norm)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Performs a single optimization step.\n",
        "\n",
        "        Arguments:\n",
        "            closure (callable, optional): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "        \"\"\"\n",
        "        \n",
        "        # TODO: use closure to compute gradient difference\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            eps = group['eps']\n",
        "            lr = group['lr']\n",
        "            damping = group['damping']\n",
        "            amplifier = group['amplifier']\n",
        "            theta = group['theta']\n",
        "            grad_dif_norm = group['grad_dif_norm']\n",
        "            param_dif_norm = group['param_dif_norm']\n",
        "            if param_dif_norm > 0 and grad_dif_norm > 0:\n",
        "                lr_new = min(lr * np.sqrt(1 + amplifier * theta), param_dif_norm / (damping * grad_dif_norm)) + eps\n",
        "            else:\n",
        "                lr_new = lr * np.sqrt(1 + amplifier * theta)\n",
        "            theta = lr_new / lr\n",
        "            group['theta'] = theta\n",
        "            group['lr'] = lr_new\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                d_p = p.grad.data\n",
        "                if group['weight_decay'] != 0:\n",
        "                    d_p.add_(group['weight_decay'], p.data)\n",
        "                p.data.add_(d_p, alpha=-lr_new)\n",
        "        return loss"
      ],
      "metadata": {
        "id": "uLMD1ZMhtULj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2,2,2,2])\n",
        "\n",
        "def ResNet34():\n",
        "    return ResNet(BasicBlock, [3,4,6,3])\n",
        "\n",
        "def ResNet50():\n",
        "    return ResNet(Bottleneck, [3,4,6,3])\n",
        "\n",
        "def ResNet101():\n",
        "    return ResNet(Bottleneck, [3,4,23,3])\n",
        "\n",
        "def ResNet152():\n",
        "    return ResNet(Bottleneck, [3,8,36,3])"
      ],
      "metadata": {
        "id": "OLRqSpbj7vBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "oYgdotbR8CSp",
        "outputId": "a1ab3001-aa2c-4527-c978-b273d433d9a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/Adaptive-gradient-descent-without-descent'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd pytorch "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GNex8ug8EcZ",
        "outputId": "6b258423-3de8-46c0-9a18-0c9043f3c624"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Adaptive-gradient-descent-without-descent/pytorch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load optimizer.py"
      ],
      "metadata": {
        "id": "LvpLMpN-8HHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load utils.py"
      ],
      "metadata": {
        "id": "MbZCfqpd8O6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load resnet.py"
      ],
      "metadata": {
        "id": "ZYEPVbZFk5Mv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "def seed_everything(seed=1029):\n",
        "    '''\n",
        "    :param seed:\n",
        "    :param device:\n",
        "    :return:\n",
        "    '''\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    # some cudnn methods can be random even after fixing the seed\n",
        "    # unless you tell it to be deterministic\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "def load_data(dataset='cifar10', batch_size=128, num_workers=4):\n",
        "    \"\"\"\n",
        "    Loads the required dataset\n",
        "    :param dataset: Can be either 'cifar10' or 'cifar100'\n",
        "    :param batch_size: The desired batch size\n",
        "    :return: Tuple (train_loader, test_loader, num_classes)\n",
        "    \"\"\"\n",
        "    print('==> Preparing data..')\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "\n",
        "    if dataset == 'cifar10':\n",
        "        # classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "        num_classes = 10\n",
        "        trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "        testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "    elif dataset == 'cifar100':\n",
        "        num_classes = 100\n",
        "        trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
        "        testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
        "    else:\n",
        "        raise ValueError('Only cifar 10 and cifar 100 are supported')\n",
        "\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "    return trainloader, testloader, num_classes\n",
        "    \n",
        "    \n",
        "def accuracy_and_loss(net, dataloader, device, criterion):\n",
        "    net.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    loss = 0\n",
        "    with torch.no_grad():\n",
        "        for data in dataloader:\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = net(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            loss += criterion(outputs, labels).cpu().item() / len(dataloader)\n",
        "\n",
        "    return correct / total, loss\n",
        "\n",
        "def save_results(losses, test_losses, train_acc, test_acc, it_train, it_test, grad_norms, method='sgd', \n",
        "                 lrs=[], experiment='cifar10_resnet18', folder='./', to_save_extra=[], prefixes_extra=[]):\n",
        "    path = f'./{folder}/{experiment}/'\n",
        "    Path(path).mkdir(parents=True, exist_ok=True)\n",
        "    to_save = [losses, test_losses, train_acc, test_acc, it_train, it_test, grad_norms, lrs] + to_save_extra\n",
        "    prefixes = ['l', 'tl', 'a', 'ta', 'itr', 'ite', 'gn', 'lr'] + prefixes_extra\n",
        "    for log, prefix in zip(to_save, prefixes):\n",
        "        np.save(f'{path}/{method}_{prefix}.npy', log)\n",
        "        \n",
        "def load_results(method, logs_path, load_lr=False):\n",
        "    path = logs_path\n",
        "    if logs_path[-1] != '/':\n",
        "        path += '/'\n",
        "    path += method + '_'\n",
        "    prefixes = ['l', 'tl', 'a', 'ta', 'itr', 'ite', 'gn']\n",
        "    if load_lr:\n",
        "        prefixes += ['lr']\n",
        "    out = [np.load(path + prefix + '.npy') for prefix in prefixes]\n",
        "    return tuple(out)\n"
      ],
      "metadata": {
        "id": "qoUxYUEZoE7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from optimizer import Adsgd\n",
        "from utils import load_data, accuracy_and_loss, save_results, seed_everything\n",
        "\n",
        "\n",
        "def run_adgd(net, n_epoch=2, amplifier=0.02, damping=1, weight_decay=0, eps=1e-8, checkpoint=125, batch_size=128, noisy_train_stat=True):\n",
        "    losses = []\n",
        "    train_acc = []\n",
        "    test_losses = []\n",
        "    test_acc = []\n",
        "    it_train = []\n",
        "    it_test = []\n",
        "    grad_norms = []\n",
        "    \n",
        "    prev_net = copy.deepcopy(net)\n",
        "    prev_net.to(device)\n",
        "    net.train()\n",
        "    prev_net.train()\n",
        "    lrs = []\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = Adsgd(net.parameters(), amplifier=amplifier, damping=damping, weight_decay=weight_decay, eps=eps)\n",
        "    prev_optimizer = Adsgd(prev_net.parameters(), weight_decay=weight_decay)\n",
        "            \n",
        "    for epoch in range(n_epoch):  # loop over the dataset multiple times\n",
        "\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            prev_optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "            prev_outputs = prev_net(inputs)\n",
        "            prev_loss = criterion(prev_outputs, labels)\n",
        "            prev_loss.backward()\n",
        "\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.compute_dif_norms(prev_optimizer)\n",
        "            prev_net.load_state_dict(net.state_dict())\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            if (i % 10) == 0:\n",
        "                if noisy_train_stat:\n",
        "                    losses.append(loss.cpu().item())\n",
        "                    it_train.append(epoch + i * batch_size / N_train)\n",
        "                lrs.append(optimizer.param_groups[0]['lr'])\n",
        "\n",
        "            if i % checkpoint == checkpoint - 1:\n",
        "                if running_loss / checkpoint < 0.01:\n",
        "                    print('[%d, %5d] loss: %.4f' %\n",
        "                          (epoch + 1, i + 1, running_loss / checkpoint), end='')\n",
        "                else:\n",
        "                    print('[%d, %5d] loss: %.3f' %\n",
        "                          (epoch + 1, i + 1, running_loss / checkpoint), end='')\n",
        "                running_loss = 0.0\n",
        "                test_a, test_l = accuracy_and_loss(net, testloader, device, criterion)\n",
        "                test_acc.append(test_a)\n",
        "                test_losses.append(test_l)\n",
        "                grad_norms.append(np.sum([p.grad.data.norm().item() for p in net.parameters()]))\n",
        "                net.train()\n",
        "                it_test.append(epoch + i * batch_size / N_train)\n",
        "                \n",
        "        if not noisy_train_stat:\n",
        "            it_train.append(epoch)\n",
        "            train_a, train_l = accuracy_and_loss(net, trainloader, device, criterion)\n",
        "            train_acc.append(train_a)\n",
        "            losses.append(train_l)\n",
        "            net.train()\n",
        "\n",
        "    del prev_net\n",
        "    return (np.array(losses), np.array(test_losses), np.array(train_acc), np.array(test_acc),\n",
        "            np.array(it_train), np.array(it_test), np.array(lrs), np.array(grad_norms))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Train ResNet18 on Cifar10 data\n",
        "    import argparse\n",
        "    from resnet import ResNet18\n",
        "    \n",
        "    # parser = argparse.ArgumentParser('Model-Agnostic Meta-Learning (MAML)')\n",
        "    # parser.add_argument('--lr_amplifier', type=float, default=0.02,\n",
        "    #     help='Coefficient alpha for multiplying the stepsize by (1+alpha) (default: 0.02).')\n",
        "    # parser.add_argument('--lr_damping', type=float, default=1.,\n",
        "    #     help='Divide the inverse smoothness by damping (default: 1.).')\n",
        "    # parser.add_argument('--weight_decay', type=float, default=0.,\n",
        "    #     help='Weight decay parameter (default: 0.).')\n",
        "    # parser.add_argument('--batch_size', type=int, default=128,\n",
        "    #     help='Number of passes over the data (default: 128).')\n",
        "    \n",
        "    # parser.add_argument('--n_epoch', type=int, default=120,\n",
        "    #     help='Number of passes over the data (default: 120).')\n",
        "    # parser.add_argument('--n_seeds', type=int, default=1,\n",
        "    #     help='Number of random seeds to run the method (default: 1).')\n",
        "    # parser.add_argument('--output_folder', type=str, default='./',\n",
        "    #     help='Path to the output folder for saving the logs (optional).')\n",
        "    \n",
        "    # args = parser.parse_args()\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    \n",
        "    N_train = 50000\n",
        "    trainloader, testloader, num_classes = load_data(batch_size=128)\n",
        "    checkpoint = len(trainloader) // 3 + 1\n",
        "    amplifier = 0.02\n",
        "    \n",
        "    n_seeds = 1\n",
        "    max_seed = 424242\n",
        "    rng = np.random.default_rng(42)\n",
        "    seeds = [rng.choice(max_seed, size=1, replace=False)[0] for _ in range(n_seeds)]\n",
        "\n",
        "    for r, seed in enumerate(seeds):\n",
        "        seed_everything(seed)\n",
        "        net = ResNet18()\n",
        "        net.to(device)\n",
        "        losses_adgd, test_losses_adgd, train_acc_adgd, test_acc_adgd, it_train_adgd, it_test_adgd, lrs_adgd, grad_norms_adgd = run_adgd(\n",
        "            net=net, n_epoch=4, amplifier=0.02, damping=1., weight_decay=0., \n",
        "            checkpoint=checkpoint, batch_size=128, noisy_train_stat=False\n",
        "        )\n",
        "        method = f'adgd_{0.02}_{1.}'\n",
        "        experiment = 'cifar10_resnet18'\n",
        "        save_results(losses_adgd, test_losses_adgd, train_acc_adgd, test_acc_adgd, it_train_adgd, it_test_adgd, lrs=lrs_adgd, \n",
        "                 grad_norms=grad_norms_adgd, method=method, experiment=experiment, folder=\"/content/Adaptive-gradient-descent-without-descent/saved_data\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2V_oND8676Dz",
        "outputId": "540dba70-ca43-415b-cfdf-e37f1974f4ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Preparing data..\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,   131] loss: 1.938[1,   262] loss: 1.550[2,   131] loss: 1.273[2,   262] loss: 1.170[3,   131] loss: 1.012[3,   262] loss: 0.930[4,   131] loss: 0.848[4,   262] loss: 0.846"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2JS-pOu7mMTY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}